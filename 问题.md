1. 传统机器学习算法：感知机，SVM，LR，softmax，Kmeans，DBSCAN，决策树（CART，ID3，C45），GBDT，RF，Adaboost，xgboost，EM，BP神经网络，朴素贝叶斯，LDA，PCA，核函数，最大熵
2. 深度学习：CNN，RNN，LSTM，常用激活函数，Adam等优化算法，梯度消失（爆炸）
3. 推荐系统：itemBasedCF，userBasedCF，冷启动，SVD（各种变形），FM，LFM
4. NLP：TF-IDF，textrank，word2vec(能推导，看过源码)，LCA，simhash
5. 常见概念：最大似然估计，最小二乘法，模型融合方法，L1L2正则（Lasso，elestic net），判别式模型与生成式模型，熵-交叉熵-KL散度，数据归一化，最优化方法（梯度下降，牛顿法，共轭梯度法），无偏估计，F1（ROC，recall，precision等），交叉验证，bias-variance-tradeoff，皮尔逊系数
6. 常见损失函数
7. SGD与BGD
8. 如何处理样本非均衡问题
9. 过拟合原因，以及解决办法
10. 如何处理数据缺失问题
11. 如何选择特征
12. L1为什么能让参数稀疏，L2为什么会让参数趋于较小值，L1优化方法
13. 各模型的优缺点，以及适用场景
14. 你了解决策树吗？回答：ID3 C4.5 优缺点 树的融合（GBDT，RF） 我的实现。
15. L1 L2了解吗？回答：L1 L2的作用，为什么有这样的作用？一般求L1的优化方法（坐标下降，LARS角回归）
16. 链表逆序你会吗？回答：非递归 递归
17. loss优化方法，说了BGD，SGD，各自优缺点，优化方向（Adam之类的）
18. LR，SVM，KNN，GBDT，XGB推导，算法细节(LR为何是sigmod，理论推导出sigmod,KNN距离度量方式，XGBoost为什么要用二阶信息不用一阶，LR和SVM对比，GBDT和XGB和LightGBM对比)。
19. CNN DNN RNN 细节以及相关问题(poll层，激活函数，梯度消失弥散问题，LSTM结构图，深度网络优势及缺点)。
20. 常见排序算法的复杂度和一些细节以及改进优化。
21. 树模型建模过程。
22. 特征选择方法。
23. 模型训练停止方法。
24. 正则化作用。
25. 模型效果评价指标。
26. AUC理解和计算方法。
27. Hadoop，Hive，Spark相关理论。
28. L_BFGS，DFP推导。
29. 弱分类器组合成强分类器的理论证明。
30.  FM，FMM，Rank_SVM算法细节。
31. map_reduce基本概念以及常见处理代码。
32. 过拟合的解决方法。
33. 各个损失函数之间区别。
34. L1,L2正则化相关问题。
35. TCP三次握手,四次挥手等细节。
36. 二叉树和排序相关算法考的最多，其次是深度优先遍历、回溯和动态规划。排序算法比较常考的是快排、堆排、归并排，还有基于快排和堆排思想的topK算法（这些都要会手写）。二叉树主要是树的前、中、后序遍历（递归和非递归，最好能手写），层次遍历，树上两个节点最远距离、树和有序链表互相转化，两节点最近公共父节点，其他还有许多，暂时想不起来了。深度优先遍历（dfs）、回溯、动态规划主要是笔试题用的多，不过最好能够手写一些经典算法，比如01背包、最长公共子序列以及其推导公式。如果感兴趣的话可以了解一下dfs、回溯、贪心以及动态规划之前的联系和区别。